{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 30)\n",
      "NotFound                                                                                               194\n",
      "third party in case of default                                                                          18\n",
      "non defaulting party in case of default                                                                 14\n",
      "substitution calculation agent in case of dispute                                                        7\n",
      "substitution calculation agent in case of default                                                        3\n",
      "other in case of dispute                                                                                 2\n",
      "substitution calculation agent in case of dispute;other in case of dispute                               2\n",
      "substitution calculation agent in case of default;third party in case of default                         1\n",
      "non defaulting party in case of default;third party in case of default                                   1\n",
      "substitution calculation agent in case of default;substitution calculation agent in case of dispute      1\n",
      "third party in case of default;substitution calculation agent in case of dispute                         1\n",
      "non defaulting party in case of default;non defaulting party in case of dispute                          1\n",
      " substitution calculation agent in case of dispute                                                       1\n",
      " non defaulting party in case of default                                                                 1\n",
      " non defaulting party in case of dispute;substitution calculation agent in case of dispute               1\n",
      "Substitution calculation agent in case of dispute                                                        1\n",
      "Name: CA_fallback_default_dispute, dtype: int64\n",
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 629) (249,)\n",
      "Training Shape (186, 629) (186,)\n",
      "Validation Shape (63, 629) (63,)\n",
      "Training Score 0.8064516129032258\n",
      "Validation Score 0.7936507936507936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dutta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score 0.989247311827957\n",
      "Validation Score 0.8253968253968254\n",
      "(140, 2)\n",
      "Testing Shape (140, 629)\n",
      "Done\n",
      "(249, 30)\n",
      "No          118\n",
      "no           53\n",
      "NO           48\n",
      "Yes          21\n",
      "yes           6\n",
      "YES           2\n",
      "NotFound      1\n",
      "Name: CA_dispute_resolution , dtype: int64\n",
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 629) (249,)\n",
      "Training Shape (186, 629) (186,)\n",
      "Validation Shape (63, 629) (63,)\n",
      "Training Score 0.8817204301075269\n",
      "Validation Score 0.8888888888888888\n",
      "Training Score 0.978494623655914\n",
      "Validation Score 0.9206349206349206\n",
      "(140, 2)\n",
      "Testing Shape (140, 629)\n",
      "(249, 30)\n",
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 364) (249,)\n",
      "Training Shape (186, 364) (186,)\n",
      "Validation Shape (63, 364) (63,)\n",
      "Training Score 0.8817204301075269\n",
      "Validation Score 0.873015873015873\n",
      "Training Score 0.9731182795698925\n",
      "Validation Score 0.8571428571428571\n",
      "(140, 2)\n",
      "Testing Shape (140, 364)\n",
      "Done\n",
      "(249, 30)\n",
      "NotFound    142\n",
      "below       100\n",
      "at            7\n",
      "Name: RD_party_A_Fitch_long_term_trigger_method, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 1000) (249,)\n",
      "(249, 1002) (249,)\n",
      "Training Shape (186, 1002) (186,)\n",
      "Validation Shape (63, 1002) (63,)\n",
      "Training Score 0.9193548387096774\n",
      "Validation Score 0.9047619047619048\n",
      "Training Score 0.9731182795698925\n",
      "Validation Score 0.9365079365079365\n",
      "Training Score 0.6236559139784946\n",
      "Validation Score 0.6031746031746031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dutta\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 2)\n",
      "(140, 4)\n",
      "(140, 1002)\n",
      "(249, 30)\n",
      "NotFound    195\n",
      "below        54\n",
      "Name: RD_party_A_Fitch_short_term_trigger_method, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 1000) (249,)\n",
      "(249, 1002) (249,)\n",
      "Training Shape (186, 1002) (186,)\n",
      "Validation Shape (63, 1002) (63,)\n",
      "Training Score 0.8924731182795699\n",
      "Validation Score 0.7936507936507936\n",
      "Training Score 0.9838709677419355\n",
      "Validation Score 0.9365079365079365\n",
      "(140, 2)\n",
      "(140, 4)\n",
      "(140, 1002)\n",
      "(249, 30)\n",
      "below       145\n",
      "NotFound     87\n",
      "at           14\n",
      "Below         2\n",
      "At            1\n",
      "Name: RD_party_A_moody_long_term_trigger_method, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 1000) (249,)\n",
      "(249, 1002) (249,)\n",
      "Training Shape (186, 1002) (186,)\n",
      "Validation Shape (63, 1002) (63,)\n",
      "Training Score 0.8494623655913979\n",
      "Validation Score 0.8095238095238095\n",
      "Training Score 0.946236559139785\n",
      "Validation Score 0.7777777777777778\n",
      "(140, 2)\n",
      "(140, 4)\n",
      "(140, 1002)\n",
      "(249, 30)\n",
      "NotFound    163\n",
      "below        86\n",
      "Name: RD_party_A_moody_short_term_trigger_method, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 1000) (249,)\n",
      "(249, 1002) (249,)\n",
      "Training Shape (186, 1002) (186,)\n",
      "Validation Shape (63, 1002) (63,)\n",
      "Training Score 0.9139784946236559\n",
      "Validation Score 0.8888888888888888\n",
      "Training Score 0.978494623655914\n",
      "Validation Score 0.8888888888888888\n",
      "(140, 2)\n",
      "(140, 4)\n",
      "(140, 1002)\n",
      "(249, 30)\n",
      "below       145\n",
      "NotFound     87\n",
      "at           14\n",
      "Below         2\n",
      "At            1\n",
      "Name: RD_party_A_SnP_long_term_trigger_method, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 1000) (249,)\n",
      "(249, 1002) (249,)\n",
      "Training Shape (186, 1002) (186,)\n",
      "Validation Shape (63, 1002) (63,)\n",
      "Training Score 0.8494623655913979\n",
      "Validation Score 0.8095238095238095\n",
      "Training Score 0.946236559139785\n",
      "Validation Score 0.7777777777777778\n",
      "(140, 2)\n",
      "(140, 4)\n",
      "(140, 1002)\n",
      "(249, 30)\n",
      "NotFound    163\n",
      "below        86\n",
      "Name: RD_party_A_SnP_short_term_trigger_method, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 1000) (249,)\n",
      "(249, 1002) (249,)\n",
      "Training Shape (186, 1002) (186,)\n",
      "Validation Shape (63, 1002) (63,)\n",
      "Training Score 0.9139784946236559\n",
      "Validation Score 0.8888888888888888\n",
      "Training Score 0.978494623655914\n",
      "Validation Score 0.8888888888888888\n",
      "(140, 2)\n",
      "(140, 4)\n",
      "(140, 1002)\n",
      "(249, 30)\n",
      "(249, 30)\n",
      "19_BK.xml\n",
      "(249, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>if_selected by the Non-defaulting Party</th>\n",
       "      <th>if_selected by the Non-defaulting</th>\n",
       "      <th>if_non-Affected Party</th>\n",
       "      <th>if_not the Affected Party</th>\n",
       "      <th>TC_Bespoke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100_bk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>To be selected by the Non-defaulting party or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101_bk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NotFound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103_bk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>To be selected by the Non-defaulting party or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104_bk</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>To be selected by the Non-defaulting party or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106_bk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NotFound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>108_bk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NotFound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>110_bk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>To be selected by the Non-defaulting party or ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File_Name  if_selected by the Non-defaulting Party  \\\n",
       "0    100_bk                                        1   \n",
       "1    101_bk                                        0   \n",
       "2    103_bk                                        1   \n",
       "3    104_bk                                        1   \n",
       "4    106_bk                                        0   \n",
       "5    108_bk                                        0   \n",
       "6    110_bk                                        0   \n",
       "\n",
       "   if_selected by the Non-defaulting  if_non-Affected Party  \\\n",
       "0                                  1                      0   \n",
       "1                                  0                      0   \n",
       "2                                  1                      0   \n",
       "3                                  1                      0   \n",
       "4                                  0                      0   \n",
       "5                                  0                      0   \n",
       "6                                  0                      1   \n",
       "\n",
       "   if_not the Affected Party  \\\n",
       "0                          1   \n",
       "1                          0   \n",
       "2                          1   \n",
       "3                          1   \n",
       "4                          1   \n",
       "5                          0   \n",
       "6                          1   \n",
       "\n",
       "                                          TC_Bespoke  \n",
       "0  To be selected by the Non-defaulting party or ...  \n",
       "1                                           NotFound  \n",
       "2  To be selected by the Non-defaulting party or ...  \n",
       "3  To be selected by the Non-defaulting party or ...  \n",
       "4                                           NotFound  \n",
       "5                                           NotFound  \n",
       "6  To be selected by the Non-defaulting party or ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249, 4) (249,)\n",
      "Training Shape (186, 4) (186,)\n",
      "Validation Shape (63, 4) (63,)\n",
      "Training Score : 0.8924731182795699\n",
      "Validation Score : 0.9206349206349206\n",
      "(140, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>if_selected by the Non-defaulting Party</th>\n",
       "      <th>if_selected by the Non-defaulting</th>\n",
       "      <th>if_non-Affected Party</th>\n",
       "      <th>if_not the Affected Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105_bk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109_bk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10_sh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11_sh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12_sh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13_sh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14_sh</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File_Name  if_selected by the Non-defaulting Party  \\\n",
       "0    105_bk                                        0   \n",
       "1    109_bk                                        0   \n",
       "2     10_sh                                        0   \n",
       "3     11_sh                                        0   \n",
       "4     12_sh                                        0   \n",
       "5     13_sh                                        0   \n",
       "6     14_sh                                        1   \n",
       "\n",
       "   if_selected by the Non-defaulting  if_non-Affected Party  \\\n",
       "0                                  0                      0   \n",
       "1                                  0                      1   \n",
       "2                                  0                      0   \n",
       "3                                  0                      0   \n",
       "4                                  0                      0   \n",
       "5                                  0                      0   \n",
       "6                                  1                      0   \n",
       "\n",
       "   if_not the Affected Party  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "5                          0  \n",
       "6                          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 4)\n",
      "(249, 30)\n",
      "No          118\n",
      "no           53\n",
      "NO           48\n",
      "Yes          21\n",
      "yes           6\n",
      "YES           2\n",
      "NotFound      1\n",
      "Name: CA_dispute_resolution , dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_BK.xml\n",
      "(249, 3)\n",
      "(249, 629) (249,)\n",
      "Training Shape (186, 629) (186,)\n",
      "Validation Shape (63, 629) (63,)\n",
      "Training Score 0.8817204301075269\n",
      "Validation Score 0.8888888888888888\n",
      "Training Score 0.978494623655914\n",
      "Validation Score 0.9206349206349206\n",
      "(140, 2)\n",
      "Testing Shape (140, 629)\n",
      "Done\n",
      "CA_dispute_resolution.csv\n",
      "CA_fallback_default_dispute.csv\n",
      "governing law.csv\n",
      "RD_party_A_Fitch_long_term_trigger_method.csv\n",
      "RD_party_A_Fitch_short_term_trigger_method.csv\n",
      "RD_party_A_moody_long_term_trigger_method.csv\n",
      "RD_party_A_moody_short_term_trigger_method.csv\n",
      "RD_party_A_SnP_long_term_trigger_method.csv\n",
      "RD_party_A_SnP_short_term_trigger_method.csv\n",
      "TC_Bespoke.csv\n",
      "(140, 11)\n"
     ]
    }
   ],
   "source": [
    "## ca default dispute\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "os.chdir(\"D:/Datasets/Brainwaves/ML/\")\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "column_name='CA_fallback_default_dispute'\n",
    "train_labels.loc[train_labels[column_name]=='third party in case of default ',column_name]='third party in case of default'\n",
    "train_labels.loc[train_labels[column_name]==' third party in case of default',column_name]='third party in case of default'\n",
    "train_labels[column_name].value_counts()\n",
    "print(train_labels[column_name].value_counts())\n",
    "# sns.countplot(train_labels[column_name])\n",
    "# plt.show()\n",
    "\n",
    "correction={\"NotFound\":0,\n",
    " \"third party in case of default\":1,\n",
    " \"non defaulting party in case of default\":2,\n",
    " \"substitution calculation agent in case of dispute\":3,\n",
    " \"substitution calculation agent in case of default\":4,\n",
    " \"other in case of dispute\":5,\n",
    " \"substitution calculation agent in case of dispute;other in case of dispute\":3,\n",
    " \"non defaulting party in case of default;third party in case of default\":2,\n",
    " \"Substitution calculation agent in case of dispute\":3,\n",
    " \"substitution calculation agent in case of default;substitution calculation agent in case of dispute\":4,\n",
    " 'third party in case of default;substitution calculation agent in case of dispute ':1,\n",
    " ' non defaulting party in case of default ':2,\n",
    "' substitution calculation agent in case of dispute':3,\n",
    " ' non defaulting party in case of dispute;substitution calculation agent in case of dispute ':2,\n",
    " 'non defaulting party in case of default;non defaulting party in case of dispute ':2,\n",
    " 'substitution calculation agent in case of default;third party in case of default':3\n",
    "}\n",
    "train_labels[column_name]=train_labels[column_name].map(correction).fillna(0)\n",
    "train_labels[column_name].value_counts()\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"par\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"calculation agent\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "\n",
    "\n",
    "encoder={\"NotFound\":0,\n",
    " \"third party in case of default\":1,\n",
    " \"non defaulting party in case of default\":2,\n",
    " \"substitution calculation agent in case of dispute\":3,\n",
    " \"substitution calculation agent in case of default\":4,\n",
    " \"other in case of dispute\":5}\n",
    "decoder=dict(zip(encoder.values(),encoder.keys()))\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\")\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"par\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"calculation agent\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values)\n",
    "print(\"Testing Shape\",X_test.shape)\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(xgbclf.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "sub.to_csv(\"public test predictions/\"+column_name+\".csv\",index=False)\n",
    "print(\"Done\")\n",
    "\n",
    "## ca dispute resolution\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "column_name='CA_dispute_resolution '\n",
    "print(train_labels[column_name].value_counts())\n",
    "# sns.countplot(train_labels[column_name])\n",
    "# plt.show()\n",
    "\n",
    "correction={\"Yes\":\"yes\",\"YES\":\"yes\",\"yes\":\"yes\",\"No\":\"no\",\"NO\":\"no\",\"no\":\"no\",\"NotFound\":\"no\"}\n",
    "train_labels[column_name]=train_labels[column_name].map(correction)\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"par\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"calculation agent\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "encoder={\"yes\":1,\"no\":0}\n",
    "decoder={1:\"yes\",0:\"no\"}\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].map(encoder).values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\")\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"par\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"calculation agent\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values)\n",
    "print(\"Testing Shape\",X_test.shape)\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(xgbclf.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "## governing law\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "train_labels['governing law'].value_counts()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "correction={\"English Law\":\"English Law\",\n",
    "            \n",
    "            \"English law\":\"English Law\",\n",
    "           \n",
    "            \"Laws of the State of New York\":\"Laws of the State of New York\",\n",
    "           \n",
    "            \"Laws of England\":\"Laws of England\",\n",
    "           \n",
    "            \"The law of state of New york\":\"Laws of the State of New York\",\n",
    "           \n",
    "            \"Laws of the state of New York\":\"Laws of the State of New York\",\n",
    "           \n",
    "            \"Laws of the state of New york\":\"Laws of the State of New York\",\n",
    "           \n",
    "            \"Laws of the state of new york\":\"Laws of the State of New York\",\n",
    "           \n",
    "            \"Laws of New South Wales\":\"Laws of New South Wales\",\n",
    "           \n",
    "            \"NotFound\":\"NotFound\",\n",
    "           \n",
    "            \"Laws of the Hong Kong Special Administrative region of the People's Republic of China \":\"NotFound\",\n",
    "           \n",
    "            \"Laws of England and Wales\":\"NotFound\",\n",
    "           \n",
    "            \"State of New South Wales\":\"Laws of New South Wales\",\n",
    "           \n",
    "            \"The Laws in New south wales\":\"Laws of New South Wales\"}\n",
    "\n",
    "\n",
    "column_name=\"governing law\"\n",
    "\n",
    "train_labels[column_name]=train_labels[column_name].map(correction)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"governing law\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "features['governing law'].value_counts()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "sns.countplot(features['governing law'])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "encoder={\"English Law\":0,\n",
    " \"Laws of the State of New York\":1,\n",
    " \"Laws of England\":2,\n",
    " \"NotFound\":3,\n",
    " \"Laws of New South Wales\":4}\n",
    "decoder=dict(zip(encoder.values(),encoder.keys()))\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].map(encoder).values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\",multi_class='ovr')\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"governing law\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values)\n",
    "print(\"Testing Shape\",X_test.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(lr.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sns.countplot(sub[column_name])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sub.to_csv(\"public test predictions/\"+column_name+\".csv\",index=False)\n",
    "print(\"Done\")\n",
    "\n",
    "# rd_party_A_Fitch_long_term_trigger_method\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "column_name='RD_party_A_Fitch_long_term_trigger_method'\n",
    "print(train_labels[column_name].value_counts())\n",
    "sns.countplot(train_labels[column_name])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "#     target_child=\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "features.iloc[1,1]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def fitch_checker(x):\n",
    "    if \"fitch\" in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def length(x):\n",
    "    return len(x.split())\n",
    "\n",
    "features['fitch_checker']=features['Text'].apply(fitch_checker)\n",
    "features['length']=features['Text'].apply(length)\n",
    "\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "encoder={\"NotFound\":0,\"below\":1,\"at\":2}\n",
    "decoder={0:\"NotFound\",1:\"below\",2:\"at\"}\n",
    "\n",
    "features[column_name]=features[column_name].map(encoder)\n",
    "\n",
    "features\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "X=np.hstack((X,features[['fitch_checker','length']]))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\",multi_class='ovr')\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "svc=LinearSVC()\n",
    "\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",svc.score(X_train,y_train))\n",
    "print(\"Validation Score\",svc.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "test_features['fitch_checker']=test_features['Text'].apply(fitch_checker)\n",
    "test_features['length']=test_features['Text'].apply(length)\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values).toarray()\n",
    "X_test=np.hstack((X_test,test_features[['fitch_checker','length']]))\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(lr.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sns.countplot(sub[column_name])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sub.to_csv(\"public test predictions/\"+column_name+\".csv\",index=False)\n",
    "\n",
    "\n",
    "# rd party A fitch short term trigger\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "column_name='RD_party_A_Fitch_short_term_trigger_method'\n",
    "print(train_labels[column_name].value_counts())\n",
    "sns.countplot(train_labels[column_name])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "#     target_child=\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "features.iloc[1,1]\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def fitch_checker(x):\n",
    "    if \"fitch\" in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def length(x):\n",
    "    return len(x.split())\n",
    "\n",
    "features['fitch_checker']=features['Text'].apply(fitch_checker)\n",
    "features['length']=features['Text'].apply(length)\n",
    "\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "encoder={\"NotFound\":0,\"below\":1}\n",
    "decoder={0:\"NotFound\",1:\"below\"}\n",
    "\n",
    "features[column_name]=features[column_name].map(encoder)\n",
    "\n",
    "features\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "X=np.hstack((X,features[['fitch_checker','length']]))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\")\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "test_features['fitch_checker']=test_features['Text'].apply(fitch_checker)\n",
    "test_features['length']=test_features['Text'].apply(length)\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values).toarray()\n",
    "X_test=np.hstack((X_test,test_features[['fitch_checker','length']]))\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(xgbclf.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "sns.countplot(sub[column_name])\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "sub.to_csv(\"public test predictions/\"+column_name+\".csv\",index=False)\n",
    "\n",
    "\n",
    "# # moody long term trigger method\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "column_name='RD_party_A_moody_long_term_trigger_method'\n",
    "print(train_labels[column_name].value_counts())\n",
    "sns.countplot(train_labels[column_name])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "correction={\"Below\":\"below\",\"below\":\"below\",\"at\":\"at\",\"At\":\"at\",\"NotFound\":\"NotFound\"}\n",
    "train_labels[column_name]=train_labels[column_name].map(correction)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "features.iloc[1,1]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def moody_checker(x):\n",
    "    if \"moody\" in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def length(x):\n",
    "    return len(x.split())\n",
    "\n",
    "features['moody_checker']=features['Text'].apply(moody_checker)\n",
    "features['length']=features['Text'].apply(length)\n",
    "\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "encoder={\"NotFound\":0,\"below\":1,\"at\":2}\n",
    "decoder={0:\"NotFound\",1:\"below\",2:\"at\"}\n",
    "\n",
    "features[column_name]=features[column_name].map(encoder)\n",
    "\n",
    "features\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "X=np.hstack((X,features[['moody_checker','length']]))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\",multi_class='ovr')\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "test_features['moody_checker']=test_features['Text'].apply(moody_checker)\n",
    "test_features['length']=test_features['Text'].apply(length)\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values).toarray()\n",
    "X_test=np.hstack((X_test,test_features[['moody_checker','length']]))\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(lr.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "sns.countplot(sub[column_name])\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "sub.to_csv(\"public test predictions/\"+column_name+\".csv\",index=False)\n",
    "\n",
    "## moody short term trigger \n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "column_name='RD_party_A_moody_short_term_trigger_method'\n",
    "print(train_labels[column_name].value_counts())\n",
    "sns.countplot(train_labels[column_name])\n",
    "plt.show()\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "features.iloc[1,1]\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def moody_checker(x):\n",
    "    if \"moody\" in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def length(x):\n",
    "    return len(x.split())\n",
    "\n",
    "features['moody_checker']=features['Text'].apply(moody_checker)\n",
    "features['length']=features['Text'].apply(length)\n",
    "\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "encoder={\"NotFound\":0,\"below\":1}\n",
    "decoder={0:\"NotFound\",1:\"below\"}\n",
    "\n",
    "features[column_name]=features[column_name].map(encoder)\n",
    "\n",
    "features\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "X=np.hstack((X,features[['moody_checker','length']]))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\")\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "test_features['moody_checker']=test_features['Text'].apply(moody_checker)\n",
    "test_features['length']=test_features['Text'].apply(length)\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values).toarray()\n",
    "X_test=np.hstack((X_test,test_features[['moody_checker','length']]))\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(lr.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "sns.countplot(sub[column_name])\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "sub.to_csv(\"public test predictions/\"+column_name+\".csv\",index=False)\n",
    "\n",
    "## snp long term\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "column_name='RD_party_A_SnP_long_term_trigger_method'\n",
    "print(train_labels[column_name].value_counts())\n",
    "sns.countplot(train_labels[column_name])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "correction={\"Below\":\"below\",\"below\":\"below\",\"at\":\"at\",\"At\":\"at\",\"NotFound\":\"NotFound\"}\n",
    "train_labels[column_name]=train_labels[column_name].map(correction)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "features.iloc[1,1]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def snp_checker(x):\n",
    "    if 'snp' in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def length(x):\n",
    "    return len(x.split())\n",
    "\n",
    "features['snp_checker']=features['Text'].apply(snp_checker)\n",
    "features['length']=features['Text'].apply(length)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "encoder={\"NotFound\":0,\"below\":1,\"at\":2}\n",
    "decoder={0:\"NotFound\",1:\"below\",2:\"at\"}\n",
    "\n",
    "features[column_name]=features[column_name].map(encoder)\n",
    "\n",
    "features\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "X=np.hstack((X,features[['snp_checker','length']]))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\",multi_class='ovr')\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "test_features['snp_checker']=test_features['Text'].apply(snp_checker)\n",
    "test_features['length']=test_features['Text'].apply(length)\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values).toarray()\n",
    "X_test=np.hstack((X_test,test_features[['snp_checker','length']]))\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(lr.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "sns.countplot(sub[column_name])\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "sub.to_csv(\"public test predictions/\"+column_name+\".csv\",index=False)\n",
    "\n",
    "## snp short term trigger method\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "column_name='RD_party_A_SnP_short_term_trigger_method'\n",
    "print(train_labels[column_name].value_counts())\n",
    "sns.countplot(train_labels[column_name])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "features.iloc[1,1]\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def snp_checker(x):\n",
    "    if 'snp' in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def length(x):\n",
    "    return len(x.split())\n",
    "\n",
    "features['snp_checker']=features['Text'].apply(snp_checker)\n",
    "features['length']=features['Text'].apply(length)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "encoder={\"NotFound\":0,\"below\":1}\n",
    "decoder={0:\"NotFound\",1:\"below\"}\n",
    "\n",
    "features[column_name]=features[column_name].map(encoder)\n",
    "\n",
    "features\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "X=np.hstack((X,features[['snp_checker','length']]))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\")\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"text\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"moody\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "#     print(datia)\n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "test_features['snp_checker']=test_features['Text'].apply(snp_checker)\n",
    "test_features['length']=test_features['Text'].apply(length)\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values).toarray()\n",
    "X_test=np.hstack((X_test,test_features[['snp_checker','length']]))\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(lr.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "sns.countplot(sub[column_name])\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "sub.to_csv(\"public test predictions/\"+column_name+\".csv\",index=False)\n",
    "\n",
    "\n",
    "## tc bespoke\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labelstrain_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "train_labels.loc[train_labels['File_Name']=='19_bk']\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "train_labels['TC_currency'].value_counts()/train_labels.shape[0]\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "train_labels['TC_currency'].unique()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['if_selected by the Non-defaulting Party']=0\n",
    "features['if_selected by the Non-defaulting']=0\n",
    "features['if_non-Affected Party']=0\n",
    "features['if_not the Affected Party']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename+\" :\")\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    # appending the file name\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "#         print(child.text)\n",
    "        if \"selected by the Non-defaulting Party\" in child.text:\n",
    "            features.loc[i,'if_selected by the Non-defaulting Party']=1\n",
    "        \n",
    "        if \"selected by the Non-defaulting\" in child.text:\n",
    "            features.loc[i,'if_selected by the Non-defaulting']=1\n",
    "            \n",
    "        if \"non-Affected Party\" in child.text:\n",
    "            features.loc[i,'if_non-Affected Party']=1\n",
    "            \n",
    "        if \"not the Affected Party\" in child.text:\n",
    "            features.loc[i,'if_not the Affected Party']=1\n",
    "            \n",
    "features=features.merge(right=train_labels[['File_Name','TC_Bespoke']],how='left',on=['File_Name'])\n",
    "\n",
    "print(features.shape)\n",
    "display(features.head(7))\n",
    "\n",
    "encoder={\"NotFound\":0,\"To be selected by the Non-defaulting party or non-affected party\":1}\n",
    "decoder=dict(zip(encoder.values(),encoder.keys()))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "X=features.iloc[:,1:-1].values\n",
    "y=features['TC_Bespoke'].map(encoder).values\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score :\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score :\",lr.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['if_selected by the Non-defaulting Party']=0\n",
    "test_features['if_selected by the Non-defaulting']=0\n",
    "test_features['if_non-Affected Party']=0\n",
    "test_features['if_not the Affected Party']=0\n",
    "\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename+\" :\")\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    \n",
    "    # appending the file name\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "#         print(child.text)\n",
    "        if \"selected by the Non-defaulting Party\" in child.text:\n",
    "            test_features.loc[i,'if_selected by the Non-defaulting Party']=1\n",
    "        \n",
    "        if \"selected by the Non-defaulting\" in child.text:\n",
    "            test_features.loc[i,'if_selected by the Non-defaulting']=1\n",
    "            \n",
    "        if \"non-Affected Party\" in child.text:\n",
    "            test_features.loc[i,'if_non-Affected Party']=1\n",
    "            \n",
    "        if \"not the Affected Party\" in child.text:\n",
    "            test_features.loc[i,'if_not the Affected Party']=1\n",
    "\n",
    "print(test_features.shape)\n",
    "display(test_features.head(7))\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "X_test=test_features.iloc[:,1:].values\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub['TC_Bespoke']=pd.Series(lr.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "sns.countplot(sub['TC_Bespoke'])\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "sub.to_csv(\"public test predictions/TC_Bespoke.csv\",index=False)\n",
    "\n",
    "# ca dispute resolution\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "train_labels=pd.read_csv(\"train_labels.csv\")\n",
    "print(train_labels.shape)\n",
    "train_labels\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "plt.xticks(rotation='90')\n",
    "column_name='CA_dispute_resolution '\n",
    "print(train_labels[column_name].value_counts())\n",
    "sns.countplot(train_labels[column_name])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "correction={\"Yes\":\"yes\",\"YES\":\"yes\",\"yes\":\"yes\",\"No\":\"no\",\"NO\":\"no\",\"no\":\"no\",\"NotFound\":\"no\"}\n",
    "train_labels[column_name]=train_labels[column_name].map(correction)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "filenames=[name for name in os.listdir(\"data/\") if \".xml\" in name]\n",
    "features=pd.DataFrame()\n",
    "c=0\n",
    "features['File_Name']=[\"NotFound\"]*len(filenames)\n",
    "features['Text']=0\n",
    "for i,filename in enumerate(filenames):\n",
    "#     print(filename,\":\")RD_party_A_SnP_short_term_trigger_method\n",
    "    tree=ET.parse(\"data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"par\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"calculation agent\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "    features.loc[i,'Text']=data\n",
    "\n",
    "features=features.merge(right=train_labels[['File_Name',column_name]],how='left',on=['File_Name'])\n",
    "print(features.shape)\n",
    "features.head()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "encoder={\"yes\":1,\"no\":0}\n",
    "decoder={1:\"yes\",0:\"no\"}\n",
    "\n",
    "tfidf=TfidfVectorizer(lowercase=True,stop_words='english',max_features=1000)\n",
    "tfidf.fit(features['Text'])\n",
    "\n",
    "X=tfidf.transform(features['Text']).toarray()\n",
    "y=features[column_name].map(encoder).values\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "y[y==0].shape[0]/y.shape[0]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,stratify=y,test_size=0.25,random_state=42)\n",
    "\n",
    "print(\"Training Shape\",X_train.shape,y_train.shape)\n",
    "print(\"Validation Shape\",X_val.shape,y_val.shape)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "lr=LogisticRegression(solver=\"liblinear\")\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",lr.score(X_train,y_train))\n",
    "print(\"Validation Score\",lr.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "xgbclf=xgb.XGBClassifier()\n",
    "\n",
    "xgbclf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score\",xgbclf.score(X_train,y_train))\n",
    "print(\"Validation Score\",xgbclf.score(X_val,y_val))\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "test_filenames=[name for name in os.listdir(\"public_test_data/\") if \".xml\" in name]\n",
    "test_features=pd.DataFrame()\n",
    "c=0\n",
    "test_features['File_Name']=[\"NotFound\"]*len(test_filenames)\n",
    "test_features['Text']=0\n",
    "for i,filename in enumerate(test_filenames):\n",
    "#     print(filename,\":\")\n",
    "    tree=ET.parse(\"public_test_data/\"+filename)\n",
    "    root=tree.getroot()\n",
    "    if filename==\"19_BK.xml\":\n",
    "        print(filename)\n",
    "        test_features.loc[i,'File_Name']=\"19_bk\"\n",
    "    else:\n",
    "        test_features.loc[i,'File_Name']=filename.split(\".\")[0]\n",
    "    \n",
    "    c=0\n",
    "    for child in root.iter(tag=root.tag.split(\"}\")[0]+\"}\"+\"par\"):\n",
    "#         print(child.tag)\n",
    "        for elem in child.iter(tag=child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "            if \"calculation agent\" in elem.text.lower():\n",
    "#                 print(elem.text)\n",
    "                target_child=child\n",
    "                c=1\n",
    "                break\n",
    "        if c==1:\n",
    "            break\n",
    "    \n",
    "    data=\"\"\n",
    "    for child in target_child.iter(tag=target_child.tag.split(\"}\")[0]+\"}\"+\"formatting\"):\n",
    "        data=data+child.text\n",
    "    \n",
    "    test_features.loc[i,'Text']=data\n",
    "\n",
    "print(test_features.shape)\n",
    "test_features.head()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "X_test=tfidf.transform(test_features['Text'].values)\n",
    "print(\"Testing Shape\",X_test.shape)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "sub=pd.DataFrame()\n",
    "sub['File_Name']=test_features['File_Name']\n",
    "sub[column_name]=pd.Series(xgbclf.predict(X_test)).map(decoder)\n",
    "sub.head()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "sns.countplot(sub[column_name])\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "sub.to_csv(\"public test predictions/CA_dispute_resolution.csv\",index=False)\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# final merger\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "test_filenames=os.listdir(\"public test predictions/\")\n",
    "\n",
    "final=pd.DataFrame()\n",
    "\n",
    "for index,filename in enumerate(os.listdir(\"public test predictions/\")):\n",
    "    if filename!='.ipynb_checkpoints':\n",
    "        print(filename)\n",
    "        temp=pd.read_csv(\"public test predictions/\"+filename)\n",
    "        if index==0:\n",
    "            final=temp\n",
    "        else:\n",
    "            final=final.merge(temp,on=['File_Name'],how='left')\n",
    "        \n",
    "print(final.shape)\n",
    "final.head()\n",
    "\n",
    "final.to_csv(\"sai_final(3).csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>CA_dispute_resolution</th>\n",
       "      <th>CA_fallback_default_dispute</th>\n",
       "      <th>governing law</th>\n",
       "      <th>RD_party_A_Fitch_long_term_trigger_method</th>\n",
       "      <th>RD_party_A_Fitch_short_term_trigger_method</th>\n",
       "      <th>RD_party_A_moody_long_term_trigger_method</th>\n",
       "      <th>RD_party_A_moody_short_term_trigger_method</th>\n",
       "      <th>RD_party_A_SnP_long_term_trigger_method</th>\n",
       "      <th>RD_party_A_SnP_short_term_trigger_method</th>\n",
       "      <th>TC_Bespoke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105_bk</td>\n",
       "      <td>no</td>\n",
       "      <td>substitution calculation agent in case of dispute</td>\n",
       "      <td>Laws of the State of New York</td>\n",
       "      <td>below</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>below</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>below</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>NotFound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109_bk</td>\n",
       "      <td>no</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>English Law</td>\n",
       "      <td>below</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>below</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>below</td>\n",
       "      <td>NotFound</td>\n",
       "      <td>To be selected by the Non-defaulting party or ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File_Name CA_dispute_resolution   \\\n",
       "0    105_bk                     no   \n",
       "1    109_bk                     no   \n",
       "\n",
       "                         CA_fallback_default_dispute  \\\n",
       "0  substitution calculation agent in case of dispute   \n",
       "1                                           NotFound   \n",
       "\n",
       "                   governing law RD_party_A_Fitch_long_term_trigger_method  \\\n",
       "0  Laws of the State of New York                                     below   \n",
       "1                    English Law                                     below   \n",
       "\n",
       "  RD_party_A_Fitch_short_term_trigger_method  \\\n",
       "0                                   NotFound   \n",
       "1                                   NotFound   \n",
       "\n",
       "  RD_party_A_moody_long_term_trigger_method  \\\n",
       "0                                     below   \n",
       "1                                     below   \n",
       "\n",
       "  RD_party_A_moody_short_term_trigger_method  \\\n",
       "0                                   NotFound   \n",
       "1                                   NotFound   \n",
       "\n",
       "  RD_party_A_SnP_long_term_trigger_method  \\\n",
       "0                                   below   \n",
       "1                                   below   \n",
       "\n",
       "  RD_party_A_SnP_short_term_trigger_method  \\\n",
       "0                                 NotFound   \n",
       "1                                 NotFound   \n",
       "\n",
       "                                          TC_Bespoke  \n",
       "0                                           NotFound  \n",
       "1  To be selected by the Non-defaulting party or ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
